<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【比赛】CCF - BDCI - 个贷违约预测【排名：13/3246】</title>
    <link href="/2021/12/03/%E3%80%90%E6%AF%94%E8%B5%9B%E3%80%91CCF%20-%20BDCI%20-%20%E4%B8%AA%E8%B4%B7%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B/"/>
    <url>/2021/12/03/%E3%80%90%E6%AF%94%E8%B5%9B%E3%80%91CCF%20-%20BDCI%20-%20%E4%B8%AA%E8%B4%B7%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前期分析"><a href="#前期分析" class="headerlink" title="前期分析"></a>前期分析</h2><div class="note note-success">            <p>比赛传送门：<a href="https://www.datafountain.cn/competitions/530/datasets">https://www.datafountain.cn/competitions/530/datasets</a></p>          </div><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><ul><li><p>人话——两个训练集预测一个测试集的二分类问题，指标为AUC。</p></li><li><p>鬼话——迁移学习，使用网上不知名处搜刮到的信贷数据（75w），以及自己的少量过往数据（仅1.5w)，来辅助预测该银行（委托方）自己的业务数据的违约情况。且所包含特征各不相同，部分同名特征同字段或同编码的含义也不完全相同，重要特征的分布也有极大的不同，这些可恶的数据居然也要用来辅助预测。</p></li></ul><h3 id="文件清单和使用说明"><a href="#文件清单和使用说明" class="headerlink" title="文件清单和使用说明"></a>文件清单和使用说明</h3><p><strong>训练数据</strong>   train_public.csv 个人贷款违约记录数据   train_internet_public.csv 某网络信用贷产品违约记录数据</p><p><strong>测试数据</strong>   test_public.csv 用于测试的数据，获取榜单排名</p><p><strong>训练数据说明</strong></p><ul><li>train_public.csv</li></ul><div class="table-container"><table><thead><tr><th>字段</th><th>字段描述</th></tr></thead><tbody><tr><td>loan_id（主键）</td><td>贷款记录唯一标识</td></tr><tr><td>user_id</td><td>借款人唯一标识</td></tr><tr><td>total_loan</td><td>贷款数额</td></tr><tr><td>year_of_loan</td><td>贷款年份</td></tr><tr><td>interest</td><td>当前贷款利率</td></tr><tr><td>monthly_payment</td><td>分期付款金额</td></tr><tr><td>grade</td><td>贷款级别</td></tr><tr><td>employment_type</td><td>所在公司类型（世界五百强、国有企业、普通企业…）</td></tr><tr><td>industry</td><td>工作领域（传统工业、商业、互联网、金融…）</td></tr><tr><td>work_year</td><td>工作年限</td></tr><tr><td>home_exist</td><td>是否有房</td></tr><tr><td>censor_status</td><td>审核情况</td></tr><tr><td>issue_month</td><td>贷款发放的月份</td></tr><tr><td>use</td><td>贷款用途类别</td></tr><tr><td>post_code</td><td>贷款人申请时邮政编码</td></tr><tr><td>region</td><td>地区编码</td></tr><tr><td>debt_loan_ratio</td><td>债务收入比</td></tr><tr><td>del_in_18month</td><td>借款人过去18个月逾期30天以上的违约事件数</td></tr><tr><td>scoring_low</td><td>借款人在贷款评分中所属的下限范围</td></tr><tr><td>scoring_high</td><td>借款人在贷款评分中所属的上限范围</td></tr><tr><td>pub_dero_bankrup</td><td>公开记录清除的数量</td></tr><tr><td>recircle_bal</td><td>信贷周转余额合计</td></tr><tr><td>recircle_util</td><td>循环额度利用率</td></tr><tr><td>initial_list_status</td><td>贷款的初始列表状态</td></tr><tr><td>earlies_credit_mon</td><td>借款人最早报告的信用额度开立的月份</td></tr><tr><td>title</td><td>借款人提供的贷款名称</td></tr><tr><td>policy_code</td><td>公开可用的策略<em>代码=1新产品不公开可用的策略</em>代码=2</td></tr><tr><td>f系列匿名特征</td><td>匿名特征f0-f4，为一些贷款人行为计数特征的处理</td></tr><tr><td>early_return</td><td>借款人提前还款次数</td></tr><tr><td>early_return_amount</td><td>贷款人提前还款累积金额</td></tr><tr><td>early_return_amount_3mon</td><td>近3个月内提前还款金额</td></tr><tr><td><strong>known_outstanding_loan</strong></td><td>借款人档案中未结信用额度的数量</td></tr><tr><td><strong>known_dero</strong></td><td>贬损公共记录的数量</td></tr><tr><td><strong>app_type</strong></td><td>是否个人申请</td></tr></tbody></table></div><ul><li>train_internet.csv</li></ul><div class="table-container"><table><thead><tr><th>字段</th><th>字段描述</th></tr></thead><tbody><tr><td>loan_id</td><td>网络贷款记录唯一标识</td></tr><tr><td>user_id</td><td>用户唯一标识</td></tr><tr><td>total_loan</td><td>网络贷款金额</td></tr><tr><td>year_of_loan</td><td>网络贷款期限（year）</td></tr><tr><td>interest</td><td>网络贷款利率</td></tr><tr><td>monthly_payment</td><td>分期付款金额</td></tr><tr><td>class</td><td>网络贷款等级</td></tr><tr><td>employment_type</td><td>所在公司类型（世界五百强、国有企业、普通企业…）</td></tr><tr><td>industry</td><td>工作领域（传统工业、商业、互联网、金融…）</td></tr><tr><td>work_year</td><td>就业年限（年）</td></tr><tr><td>house_ownership</td><td>是否有房</td></tr><tr><td>censor_status</td><td>验证状态</td></tr><tr><td>issue_date</td><td>网络贷款发放的月份</td></tr><tr><td>use</td><td>贷款用途</td></tr><tr><td>post_code</td><td>借款人邮政编码的前3位</td></tr><tr><td>region</td><td>地区编码</td></tr><tr><td>debt_loan_ratio</td><td>债务收入比</td></tr><tr><td>del_in_18month</td><td>借款人过去18个月信用档案中逾期60天内的违约事件数</td></tr><tr><td>scoring_low</td><td>借款人在信用评分系统所属的下限范围</td></tr><tr><td>scoring_high</td><td>借款人在信用评分系统所属的上限范围</td></tr><tr><td>pub_dero_bankrup</td><td>公开记录清除的数量</td></tr><tr><td>early_return</td><td>提前还款次数</td></tr><tr><td>early_return_amount</td><td>提前还款累积金额</td></tr><tr><td>early_return_amount_3mon</td><td>近3个月内提前还款金额</td></tr><tr><td>recircle_bal</td><td>信贷周转余额合计</td></tr><tr><td>recircle_util</td><td>循环额度利用率，或借款人使用的相对于所有可用循环信贷的信贷金额</td></tr><tr><td>initial_list_status</td><td>网络贷款的初始列表状态</td></tr><tr><td>earlies_credit_line</td><td>网络贷款信用额度开立的月份</td></tr><tr><td>title</td><td>借款人提供的网络贷款名称</td></tr><tr><td>policy_code</td><td>公开策略=1不公开策略=2</td></tr><tr><td><strong>f系列匿名特征</strong></td><td>匿名特征f0-f5，为一些网络贷款人行为计数特征的处理——<strong>多一个f5</strong></td></tr><tr><td><strong>sub_class</strong></td><td>网络贷款等级之子级</td></tr><tr><td><strong>work_type</strong></td><td>工作类型（公务员、企业白领、创业…）</td></tr><tr><td><strong>marriage</strong></td><td>婚姻状态（未婚、已婚、离异、丧偶）</td></tr><tr><td><strong>offsprings</strong></td><td>子女状态(无子女、学前、小学、中学、大学、工作)</td></tr><tr><td><strong>house_loan_status</strong></td><td>房屋贷款状况（无房贷、正在还房贷、已经还完房贷）</td></tr></tbody></table></div><ul><li>选手提交 submission.csv</li></ul><div class="table-container"><table><thead><tr><th>字段名</th><th>字段说明</th></tr></thead><tbody><tr><td>id</td><td>贷款记录ID</td></tr><tr><td>isDefault</td><td>是否违约（可为概率、最后我们提交的是归一化rank)</td></tr></tbody></table></div><h3 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h3><ul><li>少量的缺失值，最多缺失的特征列也不到10%</li><li><p>大量的特征格式问题</p></li><li><p>主表train_public缺失工资、有无子女等重要相关特征</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
      <category>比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>迁移学习</tag>
      
      <tag>金融</tag>
      
      <tag>风控</tag>
      
      <tag>DataFountain</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【比赛】Kaggle - G-Research Crypto Forecasting 【正在进行】</title>
    <link href="/2021/12/02/%E3%80%90%E6%AF%94%E8%B5%9B%E3%80%91Kaggle%20-%20G-Research%20Crypto%20Forecasting/"/>
    <url>/2021/12/02/%E3%80%90%E6%AF%94%E8%B5%9B%E3%80%91Kaggle%20-%20G-Research%20Crypto%20Forecasting/</url>
    
    <content type="html"><![CDATA[<h2 id="前期分析"><a href="#前期分析" class="headerlink" title="前期分析"></a>前期分析</h2><h3 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h3><p>简述：预测14款比特币未来的趋势，七七八八弄了一个较为复杂的Target，内核就是预测趋势，准确来说，是未来加密货币的随着时间的价格变化率的近似，而评估指标是加权的皮尔逊相关系数。</p><div class="note note-success">            <blockquote><p>赛题传送门：<a href="https://www.kaggle.com/c/g-research-crypto-forecasting/overview/description">G-Research Crypto Forecasting | Kaggle</a></p></blockquote>          </div><h3 id="Dataset-Structure"><a href="#Dataset-Structure" class="headerlink" title="Dataset Structure"></a>Dataset Structure</h3><p> <strong>train.csv</strong> - The training set</p><ol><li>timestamp - A timestamp for the minute covered by the row.</li><li>Asset_ID - An ID code for the cryptoasset (加密资产).</li><li>Count - The number of trades that took place this minute. 一分钟发生的交易数量。</li><li>Open - The USD price at the beginning of the minute. 起始价</li><li>High - The highest USD price during the minute. 最高价</li><li>Low - The lowest USD price during the minute. 最低价</li><li>Close - The USD price at the end of the minute. 结束价</li><li>Volume - The number of cryptoasset u units traded during the minute. 分钟总交易额</li><li>VWAP - The volume-weighted average price for the minute. </li><li>Target - 15 minute residualized returns. See the ‘Prediction and Evaluation section of this notebook for details of how the target is calculated.</li><li>Weight - Weight, defined by the competition hosts <a href="https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition">here</a></li><li>Asset_Name - Human readable Asset name.</li></ol><p><strong>example_test.csv</strong> - An example of the data that will be delivered by the time series API.</p><p><strong>example_sample_submission.csv</strong> - An example of the data that will be delivered by the time series API. The data is just copied from train.csv.</p><p><strong>asset_details.csv</strong> - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.</p><p><strong>supplemental_train.csv</strong> - After the submission period is over this file’s data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.</p><ul><li>📌 There are 14 coins in the dataset</li><li>📌 There are 4 years in the [full] dataset</li></ul><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><ul><li><a href="https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/290806">G-Research Crypto Forecasting | Kaggle</a> ——GrandMaster分享。</li><li><a href="https://cloud.tencent.com/developer/article/1120107">教程 | Kaggle网站流量预测任务第一名解决方案：从模型到代码详解时序预测 - 云+社区 - 腾讯云 (tencent.com)</a></li><li><a href="https://www.kaggle.com/yamqwe/g-research-avoid-overfit-feature-neutralization/">[G-Research] Avoid Overfit: Feature Neutralization | Kaggle</a> ——防止过拟合</li><li><a href="https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/284903">G-Research Crypto Forecasting | Kaggle</a> ——相关案例分享</li><li><a href="https://www.kaggle.com/alexfir/recreating-target">Recreating Target | Kaggle</a> —— 重建label</li></ul><h3 id="Competetion-Target"><a href="#Competetion-Target" class="headerlink" title="Competetion Target"></a>Competetion Target</h3><ul><li><p>监督学习，所需要预测目标——“Target”特征</p><ul><li><strong>Target</strong>: Residual log-returns for the asset over a 15 minute horizon.</li></ul><p><img src="/2021/12/02/%E3%80%90%E6%AF%94%E8%B5%9B%E3%80%91Kaggle%20-%20G-Research%20Crypto%20Forecasting/image-20211202000637295.png" alt="Target"></p></li></ul><div class="note note-success">            <blockquote><p>图片来源：<a href="https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition">G-Research 加密竞赛教程 |卡格尔 (kaggle.com)</a></p></blockquote>          </div><ul><li>评价指标：<span class="label label-primary">Target的加权皮尔逊相关系数</span> </li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">weighted_correlation</span>(<span class="hljs-params">a, b, weights</span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">   a:预测值</span><br><span class="hljs-string">   b:实际值</span><br><span class="hljs-string">   weights:Asset权重</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    w = np.ravel(weights)<br>    a = np.ravel(a)<br>    b = np.ravel(b)<br><br>    sum_w = np.<span class="hljs-built_in">sum</span>(w)<br>    mean_a = np.<span class="hljs-built_in">sum</span>(a * w) / sum_w<br>    mean_b = np.<span class="hljs-built_in">sum</span>(b * w) / sum_w<br>    var_a = np.<span class="hljs-built_in">sum</span>(w * np.square(a - mean_a)) / sum_w<br>    var_b = np.<span class="hljs-built_in">sum</span>(w * np.square(b - mean_b)) / sum_w<br><br>    cov = np.<span class="hljs-built_in">sum</span>((a * b * w)) / np.<span class="hljs-built_in">sum</span>(w) - mean_a * mean_b<br>    corr = cov / np.sqrt(var_a * var_b)<br><br>    <span class="hljs-keyword">return</span> corr<br></code></pre></div></td></tr></table></figure><ul><li>进一步分析，该分数重点可拆分为两点：<ol><li><strong>权重</strong>，由于是加权皮尔逊相关系数，那么权重更大的Target对相关系数的贡献越多</li><li><strong>相关系数</strong>，也即比较的是相关性，是变化率之间的联系，相当于是时序之中的斜率，也即在未来时间段看涨还是看跌。</li></ol></li></ul><h2 id="数据观测"><a href="#数据观测" class="headerlink" title="数据观测"></a>数据观测</h2>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
      <category>比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>金融</tag>
      
      <tag>时序分析</tag>
      
      <tag>Kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Data Science】决策树家族史</title>
    <link href="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/"/>
    <url>/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="决策树基础"><a href="#决策树基础" class="headerlink" title="决策树基础"></a>决策树基础</h2><h3 id="贪心决策树的案底"><a href="#贪心决策树的案底" class="headerlink" title="贪心决策树的案底"></a>贪心决策树的案底</h3><ul><li><p>最基础的弱学习器——决策树，其贪心雅号来源于他的优化算法——<strong>贪心算法</strong>，也即每一步特征切分都是<strong>局部最优，而非全局最优。</strong>大抵就是只顾当前，不顾未来，之后咋样，我管不着的算法。</p></li><li><p>一些聊胜于无的专业术语：叶节点、根结点、内部结点。</p><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/%E7%88%B1%E8%AE%B0%E4%B8%8D%E8%AE%B0%E7%B3%BB%E5%88%97-16383607496321.png" alt="决策树构架"></p></li><li><p>贪心的决策树算法分为三个部分——如何切分（切分特征与切分点的选择）、决策树的生成、决策树的剪枝。</p></li></ul><h3 id="决策树切分"><a href="#决策树切分" class="headerlink" title="决策树切分"></a>决策树切分</h3><p>首先我们先从分类问题进行切入，回归问题将循序渐进的介绍，因为决策树最开始是专职处理分类问题，后随着要求的不断更迭，决策树算法也可以处理连续型与混合型。</p><h4 id="不纯度"><a href="#不纯度" class="headerlink" title="不纯度"></a>不纯度</h4><ul><li><p>我们希望决策树的分支结点所包含的样本尽可能属于同⼀类别，即结点的 “纯度”（purity）越来越⾼。在分类树中，划分的优劣⽤不纯度度量（impurity-measure）定量分析。</p><blockquote><p>简单来说，不纯度就代表着不确定性，假如按照某种特征与切分点切分后，结果是将label五五分成，这也就代表了最混乱的状态，这类切分毫无意义，我奶奶来切都比他切得好。</p></blockquote></li><li><p>分别为<em>Entropy</em>（熵）、<em>Gini index</em>（基尼系数）、<em>Classification error</em>（误分类误差）。以下的p均代表label占总体的比例，$p_1$代表第一类label的占比。</p></li><li><p>Entropy</p><script type="math/tex; mode=display">  Entropy(p) = -\sum_{i=1}p_i*\log_2p_i</script><blockquote><p>当切分的子节点为2个时，易得该项为Cross-Entropy</p></blockquote></li><li><p>Gini index</p><script type="math/tex; mode=display">  Gini(p) = 1 -\sum_{i=1}p^2_i</script></li><li><p>Classification Error</p><script type="math/tex; mode=display">CE(p) = 1-\max(p)</script></li></ul><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/Geogebra%E4%B9%B1%E7%94%BB%E7%B3%BB%E5%88%97-16383608315661.png" alt="不纯度"></p><ul><li>决策树最终的优化⽬标是<strong>让叶结点的总不纯度最低</strong>，即对应衡量不纯度的指标最低(但实际上贪心算法是短视的,它只能保证每一步的总不纯度是最小的,无法倒退)</li></ul><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><ul><li><p>通常我们说的信息增益，是默认<em>信息熵</em>作为不纯度的度量。</p></li><li><p>定义信息增益为Gain, 父结点不纯度impurity为I(parent),子结点的总不纯度为I(child)，故：</p></li></ul><script type="math/tex; mode=display">Gain = I(parent) - I(child)</script><ul><li><p>由于I(parent)是既定的，故要使得Gain最大，实际上是需要I(child)最小，对于I(child)，其实际为条件不纯度的加权平均综合。</p></li><li><p>若以熵为不纯度，则I(child)等价于条件熵。设$H(Y|X)$代表条件熵，X表示某个具体的特征，x则表示该特征中某个具体的分类，$H(Y|X=x)$代表后验熵（注意负号），那么可知<strong>信息增益优化的目标就是让条件熵最小</strong>。</p><script type="math/tex; mode=display">H(Y|X) = \sum_{x\in X}p(x)H(Y|X=x)\\</script><script type="math/tex; mode=display">H(Y|X=x) = -\sum_{y\in Y}p(y|x)*\log_2{p(y|x)}</script></li></ul><h3 id="决策树生成（早先的两种算法）"><a href="#决策树生成（早先的两种算法）" class="headerlink" title="决策树生成（早先的两种算法）"></a>决策树生成（早先的两种算法）</h3><h4 id="树之明灯——ID3算法"><a href="#树之明灯——ID3算法" class="headerlink" title="树之明灯——ID3算法"></a>树之明灯——ID3算法</h4><ul><li>撒旦来了都说好的ID3算法原型来源于 J.R Quinlan 的博士论⽂，是早期基础理论较为完善，使用较为⼴泛的决策树模型，在此基础上 J.R Quinlan 进行优化后，陆续推出了C4.5 和 C5.0 决策树算法。</li><li><p>ID3算法的核心是在决策树各个结点应⽤<strong>信息增益</strong>准则选择特征，递归地构建决策树。具体⽅法是： </p><ol><li><p>从根结点开始，对结点计算所有可能的特征的信息增益。</p></li><li><p><strong>选择信息增益最大的特征</strong>作为切分结点的特征。</p></li><li>由该特征的不同取值建立子结点 再对每个子结点也递归的使用如根节点同样的处理方式。</li><li>直到所有特征的信息增益都小于设定的阈值（再切分的收益很小了）或没有特征可以选择为止（信息增益已经没有压榨空间了），最后得到一颗决策树。</li></ol></li><li><p>ID3算法的优缺点：</p><ul><li><p>Advantages: 开创性，他的到来为后来的各种进阶算法打下了基础。</p></li><li><p>Disadvantages（包揽了能想到的所有缺点）:</p><ul><li><strong>不能处理连续型数据。</strong></li><li><p>信息熵为基础的信息增益，如同一个鼓动多分类的罪犯，<strong>他的滔天大罪就是倡导毫无必要的多分类</strong>。因为分类个数越多，信息增益就会变高，于是乎讨喜的特征总会是那些让人直呼上帝的多分类特征。极端条件下，每个样本对应一个分类的特征占据了信息增益最大化的最优解，而这不是我们想看到的。</p></li><li><p><strong>无法很好的处理缺失值。</strong></p></li><li>没有任何的剪枝处理，<strong>严重过拟合。</strong></li></ul></li></ul></li><li>ID3如此之多的缺点，堪称经典负面教材，它赤裸裸的展示了其堪称完美的体无完肤，为后来的C4.5、C5.0等算法开创了鲜明的道路，值得谬赞。</li></ul><div class="note note-success">            <p>附一个ID3的代码（仅供参考）：</p>          </div><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calEnt</span>(<span class="hljs-params">dataSet</span>):</span><br>    n = dataSet.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># 数据集总⾏数</span><br>    iset = dataSet.iloc[:,-<span class="hljs-number">1</span>].value_counts() <span class="hljs-comment"># 标签的所有类别</span><br>    p = iset/n <span class="hljs-comment"># 每⼀类标签所占⽐</span><br>    ent = (-p*np.log2(p)).<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># 计算信息熵</span><br>    <span class="hljs-keyword">return</span> ent<br><br><span class="hljs-comment"># 创建数据集</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createDataSet</span>():</span><br>    row_data = &#123;<span class="hljs-string">&#x27;accompany&#x27;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>                <span class="hljs-string">&#x27;game&#x27;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>                <span class="hljs-string">&#x27;bad boy&#x27;</span>:[<span class="hljs-string">&#x27;yes&#x27;</span>,<span class="hljs-string">&#x27;yes&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>]&#125;<br>    dataSet = pd.DataFrame(row_data)<br>    <span class="hljs-keyword">return</span> dataSet<br>dataSet = createDataSet()<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> axis：数据集最佳切分列的索引</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 选择最优的列进⾏切分</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bestSplit</span>(<span class="hljs-params">dataSet</span>):</span><br>    baseEnt = calEnt(dataSet) <span class="hljs-comment"># 计算原始熵</span><br>    bestGain = <span class="hljs-number">0</span> <span class="hljs-comment"># 初始化信息增益</span><br>    axis = -<span class="hljs-number">1</span> <span class="hljs-comment"># 初始化最佳切分列，</span><br>标签列<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dataSet.shape[<span class="hljs-number">1</span>]-<span class="hljs-number">1</span>): <span class="hljs-comment"># 对特征的每⼀列进⾏</span><br>    循环<br>    levels= dataSet.iloc[:,i].value_counts().index <span class="hljs-comment"># 提取出当前列的所有</span><br>    取值<br>    ents = <span class="hljs-number">0</span> <span class="hljs-comment"># 初始化⼦节点的信息</span><br>    熵<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> levels: <span class="hljs-comment"># 对当前列的每⼀个取值进⾏循环</span><br>        childSet = dataSet[dataSet.iloc[:,i]==j] <span class="hljs-comment"># 某⼀个⼦节点的dataframe</span><br>        ent = calEnt(childSet) <span class="hljs-comment"># 计算某⼀个⼦节点的信息熵</span><br>        ents += (childSet.shape[<span class="hljs-number">0</span>]/dataSet.shape[<span class="hljs-number">0</span>])*ent <span class="hljs-comment"># 计算当前列的信息熵</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;第<span class="hljs-subst">&#123;i&#125;</span>列的信息熵为<span class="hljs-subst">&#123;ents&#125;</span>&#x27;</span>)<br>        infoGain = baseEnt-ents <span class="hljs-comment"># 计算当前列的信息增益</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;第<span class="hljs-subst">&#123;i&#125;</span>列的信息增益为<span class="hljs-subst">&#123;infoGain&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">if</span> (infoGain &gt; bestGain):<br>            bestGain = infoGain <span class="hljs-comment"># 选择最⼤信息增益</span><br>            axis = i <span class="hljs-comment"># 最⼤信息增益所在列</span><br>            的索引<br>            <span class="hljs-keyword">return</span> axis<br>        <br><span class="hljs-comment"># bestSplit(dataSet) # 返回的结果为0，即选择第0列来切分数据集</span><br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数功能：按照给定的列划分数据集</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集</span><br><span class="hljs-string"> axis：指定的列索引</span><br><span class="hljs-string"> value：指定的属性值</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> redataSet：按照指定列索引和属性值切分后的数据集</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mySplit</span>(<span class="hljs-params">dataSet,axis,value</span>):</span><br>    col = dataSet.columns[axis]<br>    redataSet = dataSet.loc[dataSet[col]==value,:].drop(col,axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> redataSet<br><br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数功能：基于最⼤信息增益切分数据集，递归构建决策树</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集(最有⼀列是标签)</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> myTree：字典形式的树</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createTree</span>(<span class="hljs-params">dataSet</span>):</span><br>    featlist = <span class="hljs-built_in">list</span>(dataSet.columns) <span class="hljs-comment"># 提取出数据集所有的列</span><br>    classlist = dataSet.iloc[:,-<span class="hljs-number">1</span>].value_counts() <span class="hljs-comment"># 获取最后⼀列类标签</span><br>    <span class="hljs-comment"># 判断最多标签数⽬是否等于数据集⾏数，或者数据集是否只有⼀列</span><br>    <span class="hljs-keyword">if</span> classlist[<span class="hljs-number">0</span>]==dataSet.shape[<span class="hljs-number">0</span>] <span class="hljs-keyword">or</span> dataSet.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> classlist.index[<span class="hljs-number">0</span>] <span class="hljs-comment"># 如果是，返回类标签</span><br>    axis = bestSplit(dataSet) <span class="hljs-comment"># 确定出当前最佳切分列的索引</span><br>    bestfeat = featlist[axis] <span class="hljs-comment"># 获取该索引对应的特征</span><br>    myTree = &#123;bestfeat:&#123;&#125;&#125; <span class="hljs-comment"># 采⽤字典嵌套的⽅式存储树信息</span><br>    <span class="hljs-keyword">del</span> featlist[axis] <span class="hljs-comment"># 删除当前特征</span><br>    valuelist = <span class="hljs-built_in">set</span>(dataSet.iloc[:,axis]) <span class="hljs-comment"># 提取最佳切分列所有属性值</span><br>    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> valuelist: <span class="hljs-comment"># 对每⼀个属性值递归建</span><br>        树<br>        myTree[bestfeat][value] = createTree(mySplit(dataSet,axis,value))<br>        <span class="hljs-keyword">return</span> myTree<br>    <br>myTree = createTree(dataSet)<br>myTree<br><br></code></pre></div></td></tr></table></figure><h4 id="改良乌龟——C4-5算法"><a href="#改良乌龟——C4-5算法" class="headerlink" title="改良乌龟——C4.5算法"></a>改良乌龟——C4.5算法</h4><p>C4.5算法改用信息增益比最大化来切分叶节点。</p><ul><li>对于D数据集的a特征，Gain_Ratio(D,a)为信息增益比,Gain(D,a)为信息增益,IV(a)为该特征的信息熵,p(i)为i特征分类的样本占比。</li></ul><script type="math/tex; mode=display">Gain\_Ratio(D,a) = \frac{Gain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">IV(a) = -\sum_{i\in{a}}p(i)\log_2p(i)</script><ul><li>IV(a)作为该特征的固有值(intrinsic value)。当a的取值i越多时，IV越大。并从总体而言，<strong>增益率准则会分类较少的特征有所偏好</strong>。</li><li><strong>启发式的应用</strong>——该算法不是将所有特征的信息增益比计算出来直接比大小，而是先算出信息增益，再根据信息增益的大小，<strong>选出几个信息增益靠前的，再计算其中信息增益比最高的。</strong></li></ul><p>C4.5算法中加入了连续变量的处理</p><ul><li>如果输入的特征是连续型变量，判定完毕后，将其按照升序排列，并从相邻2个数之间选取平均值作为切分备选点。N个值就有N-1个切分点，每个切分点都代表一种切分的可能性，也即将N个数字进行N-1种2分，并将两个连续值域对应成两个离散的类别。假设连续特征里有一万个大小不等的数，那就有9999中切分方法，显然这带来的就是一个让人心脏骤停的后果 —— <strong>慢！</strong></li></ul><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/image-20211205224342623.png" alt="C4.5连续型变量切分逻辑"></p><p>C4.5算法的优缺点</p><ul><li><p>Advantages:</p><ul><li>可处理连续型</li><li>提供了启发式</li><li>可用于分箱</li><li>不会像ID3那样，鼓励多分类，Gain Ratio对多分类趋势进行了打击。</li><li>可以处理缺失值了，将缺失值当做一种特殊的分类处理。</li></ul></li><li><p>Disadvantages:</p><ul><li>慢！</li><li><p><strong>过拟合呢？哦！弟中弟啊，怎么还不解决那该死的过拟合！</strong></p></li><li><p>另外可以稍加延伸的是，多分类特征编码的问题也没有解决，Onehot的维度灾难问题悬而未决。</p></li><li>回归问题怎么办？只能做分类问题吗？我堂堂决策树！难道！就不能解决回归问题吗！你说啊！</li></ul></li></ul><h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><h4 id="预剪枝与后剪枝"><a href="#预剪枝与后剪枝" class="headerlink" title="预剪枝与后剪枝"></a>预剪枝与后剪枝</h4><div class="note note-success">            <p>众所周知，当假设空间中有不同复杂度的模型时，基于奥卡姆剃刀——如无必要、勿增实体的原则，我们通常会选简单的模型。</p>          </div><p>那么对于一颗力求简洁的树而言，枝繁叶茂就是它的头等大罪，故树深(max_depth)与叶子数(num_leafs)两个参数是优化的目标。高则过拟合，低则欠拟合，我们通常会想取乎其中，但对于决策树问题，它不加限制，往往只会是过拟合，所以我们需要限制它的生长。</p><p>常⻅的剪枝策略有 <strong>”预剪枝“（Pre-Pruning）和 ”后剪枝“（Post-Pruning）</strong></p><ul><li><p>预剪枝：如果当前结点的划分不能带来决策树泛化性能(分类问题通常是：Accuracy)的提升，则停止划分。</p></li><li><p>后剪枝：先生成完整的树，然后从下到上对非叶节点进行考察，如果将该结点对应的子树剪枝为一个叶节点能带来泛化能力提升，则替换。</p><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/image-20211206021032924.png" alt="剪枝对比"></p></li></ul><h4 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h4><p>随着以上的剪枝思想的考量，应运而生的是<strong>极为重要的基学习器——CART算法</strong></p><ul><li>二叉树，没有该死的多叉树了！二叉树原则：条件成立向左，反之向右。</li><li>Label可以是离散型，也可以是连续型，终于可以解决回归问题了！</li><li><p>连续型数据不需要离散化，C4.5算法那个乌龟一样慢的分箱思路终于宣告结束。</p></li><li><p>CART分类树：</p><ul><li><strong>Gini准则</strong>，也即用gini系数平换信息增益中的信息熵（不纯度的度量变换）。</li><li>对于分类问题，基于子节点的加权平均错误率与父节点的比烂，一轮开摆后，如果行情见长，就断子绝孙。</li></ul></li><li><p>CART回归树：</p><ul><li><p><strong>平方误差最小化准则</strong>，先遍历特征j，找到切分点s。在此特征此切分的情况下，计算一个平方误差最小值，并记录下来。设某个连续特征有n个数，则该特征需要正常需要计算(n-1)次，诶？这不是又成乌龟了！</p></li><li><p>这种划分问题被称之为NP难问题，<strong>实际的优化具备启发式</strong>，总之是可以更快！但在这里不展开了。</p></li><li><p>以下为优化目标解析式, 其中C1为左子树，C2为右子树, yi为要搜寻的最优y值，其实就是切分后子树的y样本均值（算术平均值就是优化的数学期望）：</p><script type="math/tex; mode=display">\min_{j,s}[\min_{c1}\sum_{x_i\in{R_1(j,s)}}(y_i-c_1)^2+\min_{c2}\sum_{x_i\in{R_2(j,s)}}(y_i-c_1)^2]</script></li><li><p>其实扯那么多也就是看所有特征的所有切分，哪个能让（左子树离差平方和+右子树离差平方和）最小。唯才是举！你有货你就来！</p></li></ul></li></ul><h3 id="决策树及三大基本算法小结"><a href="#决策树及三大基本算法小结" class="headerlink" title="决策树及三大基本算法小结"></a>决策树及三大基本算法小结</h3><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/image-20211206020548091-16387275498272.png" alt="决策树Typora表"></p><h4 id="决策树优点"><a href="#决策树优点" class="headerlink" title="决策树优点"></a>决策树优点</h4><ol><li>白盒模型，有解释性，因为树可以画出来，不像云山雾绕的神经网络。</li><li>树模型只管相对大小，不需要搞花里胡哨（归一化、标准化）</li><li>缺失值也可以不管！统一看做一个特殊类。</li><li>可以同时处理数值型和分类型。</li><li><p>预测时的计算成本相比其他的算法要低，为训练样本数的对数$\log{n}$。</p></li><li><p>可以处理多输出问题，也就是不止一个label，预测对象可以是一个label矩阵。</p></li><li>可以使⽤统计测试验证模型，这让我们可以考虑模型的可靠性。</li></ol><h4 id="决策树缺点"><a href="#决策树缺点" class="headerlink" title="决策树缺点"></a>决策树缺点</h4><ol><li>过拟合——方差太大。</li><li>欠拟合——存在偏差，究其根本就是不稳定性，数据集很小的变化可能会导致生成一个截然不同的树。</li><li>贪婪算法，优化目标仅局部而非全局。</li><li>部分领域不擅长，例如XOR，奇偶校验或多路复⽤器问题</li><li>如果标签中的某些类占主导地位，会创建偏向主导类的树——数据样本不平衡时，该算法听风是雨，人云亦云。</li><li>决策树是非参数模型,我们并无法通过决策树得知任何关于分布的参数。</li></ol><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ul><li><p>虽然说存在剪枝的操作可以一定程度上限制过拟合，但实际决策树仍然是一个极度过拟合的算法。我们<strong>需要一些其他的方式来防止过拟合</strong>。</p></li><li><p>单颗决策树如此的不稳定，多颗树集成学习的犯罪动机初见端倪。</p></li><li>基于决策树算法，后续引入的两大集成思路也慢慢浮出水面——<strong>Bagging、Boosting。</strong>而这些也是现在工业界炙手可热的算法框架。</li></ul><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><ul><li><p>Bagging存在对袋外数据(out of bags)的利用，也就是藏了一手数据，用来防止过拟合，类似于交叉验证的思想来选择训练集。</p></li><li><p>Bagging用来筛选数据的方法是<strong>重复放回采样</strong>，采样次数通常就是数据集样本个数次，假设有n个样本，那么采样的比例（每个样本被抽中的概率）可以计算如下：</p><script type="math/tex; mode=display">\lim\limits_{n\to+\infty}(1-(1-\frac{1}{n})^n) = 1-\frac{1}{e}\approx0.632</script><blockquote><p>随机采样(Bootstrap Sampling)：从n个样本里重复有放回的采样n次，然后重复采样的部分删去。</p></blockquote></li><li><p>Bagging是<strong>并行式集成学习</strong>的著名代表，每进行一次Bootstrap Sampling，都会生成不一样的采样集。<strong>假设基学习器为决策树</strong>，那么对每个采样集，都会生成一颗决策树。</p><ul><li>对分类任务，使用Voting（投票法），实际预测结果为所有决策树的众数。</li><li>对回归任务，使用Model-Averaging(平均法)，实际预测结果为所有决策树的平均数。</li></ul></li></ul><h4 id="Bagging基本思想小结"><a href="#Bagging基本思想小结" class="headerlink" title="Bagging基本思想小结"></a>Bagging基本思想小结</h4><ol><li><p>给定一个弱学习器和一个训练集</p></li><li><p>弱学习器要弱，不能太准。</p></li><li><p>将该学习算法使用多次, 再进行投票or平均。</p></li><li><p>最后结果准确率将得到提高。</p></li><li>Bagging<strong>通过降低基分类器的方差</strong>，<strong>改善了泛化误差</strong>。如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差。</li><li>Bagging算法办事，讲究公平，公平，还是TM的公平！每个样本被选中的概率与权重都相同！（Boosting：我怀疑你在内涵我）</li></ol><h4 id="Bagging手下大将——RandomForest"><a href="#Bagging手下大将——RandomForest" class="headerlink" title="Bagging手下大将——RandomForest"></a>Bagging手下大将——RandomForest</h4><p>随机森林的核心是随机，其随机体现在三个方面：</p><ol><li><p>行随机：Bagging头号唯粉——每颗决策树的采样集随机。</p></li><li><p>列随机：弱水三千，仅取两千——通过设置超参数，可调节特征选择的比例，也即每颗决策树的列也各有不同。</p></li><li><p>切分随机：偶尔任性，超越贪婪——对于每颗子树（通常都是CART），不一定选择最优的切分特征来切分，而是在排名靠前的几个特征里按照一定权重来选择，大部分时候选最优，偶尔选次优的特征来切分，以此来脱出贪婪算法的牢笼，以求寻找到全局最优的切分可能（当然大概率更差）。</p></li></ol><p>随机森林优缺点：</p><ul><li>Advantages: <ul><li>随机森林在运算量没有显著提高的前提下提高了预测精度。</li><li>有效降低了过拟合，也即降低了泛化误差中的方差部分。</li><li>在sklearn中，可以借助API来观察特征重要性。</li></ul></li><li>Disadvantages:<ul><li>森林中的决策树个数很多时，训练需要的时间和空间会较大。</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
      <category>模型选择</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>决策树</tag>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Data Science】Python机器学习实用库（一）</title>
    <link href="/2021/11/02/%E3%80%90Data%20Science%E3%80%91Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2021/11/02/%E3%80%90Data%20Science%E3%80%91Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>库名</th><th>功能</th></tr></thead><tbody><tr><td>Numpy(SciPy) / Pandas / Matplotlib</td><td>三剑客，统计基础</td></tr><tr><td>Sklearn / Statsmodels / Eli5</td><td>结构化ML常用框架</td></tr><tr><td>Tqdm / Re / Gc / Os / Warning / Time</td><td>常用的基础辅助库</td></tr><tr><td>Imbalanced-learn / Missingpy</td><td>特征工程常用库</td></tr><tr><td>Seaborn / Pyecharts / Plotly</td><td>可视化常用库</td></tr><tr><td>Pandas_profiling</td><td>数据分析看板库</td></tr><tr><td>Optuna / Scikit-optimize / Skopt / HyperOpt</td><td>贝叶斯优化库</td></tr><tr><td>Pytorch / Tersonflow / Keras / Theano / Sonnet</td><td>DL常用框架</td></tr><tr><td>Xgboost / Catboost / Lightgbm / Ngboost</td><td>四大集成树模型</td></tr><tr><td>PyFlux</td><td>时序模型</td></tr><tr><td>Pycaret / MLJAR AutoML</td><td>AutoML库</td></tr><tr><td>Featuretools</td><td>特征构造库</td></tr><tr><td>Faker</td><td>假数据生成</td></tr><tr><td>Scrapy</td><td>数据采集</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Data Science】常用统计指标</title>
    <link href="/2021/10/09/%E3%80%90Data%20Science%E3%80%91%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%A0%87/"/>
    <url>/2021/10/09/%E3%80%90Data%20Science%E3%80%91%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h2 id="基础指标"><a href="#基础指标" class="headerlink" title="基础指标"></a>基础指标</h2><h3 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a>平均数</h3><ul><li><p>任一数据的变动都会引起该数值的变动，<strong>受极端值影响较大</strong>。</p></li><li><p>通常而言我们不会使用平均数来填充缺失值。</p><ul><li>均值会因偏态而无法准确反应样本实际情况，使用要慎重。</li></ul></li></ul><ul><li>平均数需要有实际意义，注意辛普森悖论。</li></ul><h3 id="众数"><a href="#众数" class="headerlink" title="众数"></a>众数</h3><ul><li><p>分类问题中常用，偷懒时，可用于离散型缺失值填充。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">series.fillna(series.mode()[<span class="hljs-number">0</span>],inplace = <span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure></li><li><p>众数也即频数，频数通常在特征构造中时常使用，作为一种特殊的编码(Frequence Encoder）。</p></li></ul><h3 id="百分位数"><a href="#百分位数" class="headerlink" title="百分位数"></a>百分位数</h3><ul><li><p>中位数：连续问题中常用此来观测数据的偏态，且可用于<strong>对完全随机缺失的连续性特征进行填充</strong>。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">series.fillna(series.median().inplace = <span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure></li><li><p>其他百分位数</p><ul><li>分箱</li><li>异常值阈值</li></ul></li></ul><h3 id="相对数"><a href="#相对数" class="headerlink" title="相对数"></a>相对数</h3><ul><li>增量</li><li>比率</li></ul><h2 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h2><h3 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h3><h3 id="方差-Var"><a href="#方差-Var" class="headerlink" title="方差(Var)"></a>方差(Var)</h3><h3 id="标准差-Std"><a href="#标准差-Std" class="headerlink" title="标准差(Std)"></a>标准差(Std)</h3><h3 id="协方差-Cov"><a href="#协方差-Cov" class="headerlink" title="协方差(Cov)"></a>协方差(Cov)</h3><h3 id="相关系数-Corr"><a href="#相关系数-Corr" class="headerlink" title="相关系数(Corr)"></a>相关系数(Corr)</h3><h3 id="可决系数-R-2"><a href="#可决系数-R-2" class="headerlink" title="可决系数(R^2)"></a>可决系数(R^2)</h3><h3 id="离差平方和-SST"><a href="#离差平方和-SST" class="headerlink" title="离差平方和(SST)"></a>离差平方和(SST)</h3><h3 id="残差-组间-平方和-SSE"><a href="#残差-组间-平方和-SSE" class="headerlink" title="残差(组间)平方和(SSE)"></a>残差(组间)平方和(SSE)</h3><h3 id="回归-组内-平方和-SSR"><a href="#回归-组内-平方和-SSR" class="headerlink" title="回归(组内)平方和(SSR)"></a>回归(组内)平方和(SSR)</h3><h3 id="泛化误差三大组成——噪音-偏差-方差"><a href="#泛化误差三大组成——噪音-偏差-方差" class="headerlink" title="泛化误差三大组成——噪音\偏差\方差"></a>泛化误差三大组成——噪音\偏差\方差</h3><h2 id="分类问题——混淆矩阵（二分类为基础）"><a href="#分类问题——混淆矩阵（二分类为基础）" class="headerlink" title="分类问题——混淆矩阵（二分类为基础）"></a>分类问题——混淆矩阵（二分类为基础）</h2><h3 id="TP、FP、TN、FN"><a href="#TP、FP、TN、FN" class="headerlink" title="TP、FP、TN、FN"></a>TP、FP、TN、FN</h3><h3 id="TPR、FPR、TNR、FNR"><a href="#TPR、FPR、TNR、FNR" class="headerlink" title="TPR、FPR、TNR、FNR"></a>TPR、FPR、TNR、FNR</h3><h3 id="第一类错误、第二类错误、Accuracy、Precision、Recall"><a href="#第一类错误、第二类错误、Accuracy、Precision、Recall" class="headerlink" title="第一类错误、第二类错误、Accuracy、Precision、Recall"></a>第一类错误、第二类错误、Accuracy、Precision、Recall</h3><h3 id="f1-score、fn-score"><a href="#f1-score、fn-score" class="headerlink" title="f1-score、fn-score"></a>f1-score、fn-score</h3><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><h3 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h3><h2 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h2><h3 id="显著性、置信度、置信区间"><a href="#显著性、置信度、置信区间" class="headerlink" title="显著性、置信度、置信区间"></a>显著性、置信度、置信区间</h3><h3 id="F、t、N、卡方检验值"><a href="#F、t、N、卡方检验值" class="headerlink" title="F、t、N、卡方检验值"></a>F、t、N、卡方检验值</h3><h3 id="熵、交叉熵、KL散度"><a href="#熵、交叉熵、KL散度" class="headerlink" title="熵、交叉熵、KL散度"></a>熵、交叉熵、KL散度</h3><h3 id="极大似然估计量与最大后验估计量"><a href="#极大似然估计量与最大后验估计量" class="headerlink" title="极大似然估计量与最大后验估计量"></a>极大似然估计量与最大后验估计量</h3>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>评价指标</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
