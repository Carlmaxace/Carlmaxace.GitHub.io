<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>决策树家族史</title>
    <link href="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/"/>
    <url>/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="决策树基础"><a href="#决策树基础" class="headerlink" title="决策树基础"></a>决策树基础</h2><h3 id="贪心决策树的案底"><a href="#贪心决策树的案底" class="headerlink" title="贪心决策树的案底"></a>贪心决策树的案底</h3><ul><li><p>最基础的弱学习器——决策树，其贪心雅号来源于他的优化算法——<strong>贪心算法</strong>，也即每一步特征切分都是<strong>局部最优，而非全局最优。</strong>大抵就是只顾当前，不顾未来，之后咋样，我管不着的算法。</p></li><li><p>一些聊胜于无的专业术语：叶节点、根结点、内部结点。</p><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/%E7%88%B1%E8%AE%B0%E4%B8%8D%E8%AE%B0%E7%B3%BB%E5%88%97-16383607496321.png" alt="决策树构架"></p></li><li><p>贪心的决策树算法分为三个部分——如何切分（切分特征与切分点的选择）、决策树的生成、决策树的剪枝。</p></li></ul><h3 id="决策树切分"><a href="#决策树切分" class="headerlink" title="决策树切分"></a>决策树切分</h3><p>首先我们先从分类问题进行切入，回归问题将循序渐进的介绍，因为决策树最开始是专职处理分类问题，后随着要求的不断更迭，决策树算法也可以处理连续型与混合型。</p><h4 id="不纯度"><a href="#不纯度" class="headerlink" title="不纯度"></a>不纯度</h4><ul><li><p>我们希望决策树的分支结点所包含的样本尽可能属于同⼀类别，即结点的 “纯度”（purity）越来越⾼。在分类树中，划分的优劣⽤不纯度度量（impurity-measure）定量分析。</p><blockquote><p>简单来说，不纯度就代表着不确定性，假如按照某种特征与切分点切分后，结果是将label五五分成，这也就代表了最混乱的状态，这类切分毫无意义，我奶奶来切都比他切得好。</p></blockquote></li><li><p>分别为<em>Entropy</em>（熵）、<em>Gini index</em>（基尼系数）、<em>Classification error</em>（误分类误差）。以下的p均代表label占总体的比例，$p_1$代表第一类label的占比。</p></li><li><p>Entropy</p><script type="math/tex; mode=display">  Entropy(p) = -\sum_{i=1}p_i*\log_2p_i</script><blockquote><p>当切分的子节点为2个时，易得该项为Cross-Entropy</p></blockquote></li><li><p>Gini index</p><script type="math/tex; mode=display">  Gini(p) = 1 -\sum_{i=1}p^2_i</script></li><li><p>Classification Error</p><script type="math/tex; mode=display">CE(p) = 1-\max(p)</script></li></ul><p><img src="/2021/12/01/%E3%80%90Data%20Science%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%B6%E6%97%8F%E5%8F%B2/Geogebra%E4%B9%B1%E7%94%BB%E7%B3%BB%E5%88%97-16383608315661.png" alt="不纯度"></p><ul><li>决策树最终的优化⽬标是<strong>让叶结点的总不纯度最低</strong>，即对应衡量不纯度的指标最低(但实际上贪心算法是短视的,它只能保证每一步的总不纯度是最小的,无法倒退)</li></ul><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><ul><li><p>通常我们说的信息增益，是默认<em>信息熵</em>作为不纯度的度量。</p></li><li><p>定义信息增益为Gain, 父结点不纯度impurity为I(parent),子结点的总不纯度为I(child)，故：</p></li></ul><script type="math/tex; mode=display">Gain = I(parent) - I(child)</script><ul><li><p>由于I(parent)是既定的，故要使得Gain最大，实际上是需要I(child)最小，对于I(child)，其实际为条件不纯度的加权平均综合。</p></li><li><p>若以熵为例，则其等价于条件熵——$H(Y|X)$代表<em>条件熵</em>，$H(Y|X=x)$代表后验熵（注意负号），<strong>信息增益优化的目标就是让条件熵最小</strong>。</p><script type="math/tex; mode=display">H(Y|X) = \sum_{x\in X}p(x)H(Y|X=x)\\\\H(Y|X=x) = -\sum_{y\in Y}p(y|x)*\log_2{p(y|x)}</script></li></ul><h3 id="决策树生成（三大决策树算法）"><a href="#决策树生成（三大决策树算法）" class="headerlink" title="决策树生成（三大决策树算法）"></a>决策树生成（三大决策树算法）</h3><h4 id="算法明灯——ID3算法"><a href="#算法明灯——ID3算法" class="headerlink" title="算法明灯——ID3算法"></a>算法明灯——ID3算法</h4><ul><li>撒旦来了都说好的ID3算法原型来源于 J.R Quinlan 的博士论⽂，是早期基础理论较为完善，使用较为⼴泛的决策树模型，在此基础上 J.R Quinlan 进行优化后，陆续推出了C4.5 和 C5.0 决策树算法。</li><li><p>ID3算法的核心是在决策树各个结点应⽤<strong>信息增益</strong>准则选择特征，递归地构建决策树。具体⽅法是： </p><ol><li><p>从根结点开始，对结点计算所有可能的特征的信息增益。</p></li><li><p><strong>选择信息增益最大的特征</strong>作为切分结点的特征。</p></li><li>由该特征的不同取值建立子结点 再对每个子结点也递归的使用如根节点同样的处理方式。</li><li>直到所有特征的信息增益都小于设定的阈值（再切分的收益很小了）或没有特征可以选择为止（信息增益已经没有压榨空间了），最后得到一颗决策树。</li></ol></li><li><p>ID3算法的优缺点：</p><ul><li><p>Advantages: 开创性，他的到来为后来的各种进阶打下了基础（后浪太强，该参照系已经哭晕在了沙滩上）</p></li><li><p>Disadvantages（包揽了你能想到的所有缺点）:</p><ul><li><strong>不能处理连续型数据。</strong></li><li><p>信息熵为基础的信息增益，如同一个鼓动多分类的罪犯，<strong>他的滔天大罪就是倡导毫无必要的多分类</strong>。因为分类个数越多，信息增益就会变高，于是乎讨喜的特征总会是那些让人直呼上帝的多分类特征。极端条件下，每个样本对应一个分类的特征占据了信息增益最大化的最优解，而这不是我们想看到的。</p></li><li><p><strong>无法很好的处理缺失值。</strong></p></li><li>没有任何的剪枝处理，<strong>严重过拟合。</strong></li></ul></li></ul></li><li>ID3如此之多的缺点，堪称经典负面教材，它赤裸裸的展示了其堪称完美的体无完肤，为后来的C4.5、C5.0等算法开创了鲜明的道路，值得谬赞。</li></ul><div class="note note-success">            <p>附一个ID3的代码（仅供参考）：</p>          </div><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs Python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calEnt</span>(<span class="hljs-params">dataSet</span>):</span><br>    n = dataSet.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># 数据集总⾏数</span><br>    iset = dataSet.iloc[:,-<span class="hljs-number">1</span>].value_counts() <span class="hljs-comment"># 标签的所有类别</span><br>    p = iset/n <span class="hljs-comment"># 每⼀类标签所占⽐</span><br>    ent = (-p*np.log2(p)).<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># 计算信息熵</span><br>    <span class="hljs-keyword">return</span> ent<br><br><span class="hljs-comment"># 创建数据集</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createDataSet</span>():</span><br>    row_data = &#123;<span class="hljs-string">&#x27;accompany&#x27;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],<br>                <span class="hljs-string">&#x27;game&#x27;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],<br>                <span class="hljs-string">&#x27;bad boy&#x27;</span>:[<span class="hljs-string">&#x27;yes&#x27;</span>,<span class="hljs-string">&#x27;yes&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>,<span class="hljs-string">&#x27;no&#x27;</span>]&#125;<br>    dataSet = pd.DataFrame(row_data)<br>    <span class="hljs-keyword">return</span> dataSet<br>dataSet = createDataSet()<br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> axis：数据集最佳切分列的索引</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 选择最优的列进⾏切分</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bestSplit</span>(<span class="hljs-params">dataSet</span>):</span><br>    baseEnt = calEnt(dataSet) <span class="hljs-comment"># 计算原始熵</span><br>    bestGain = <span class="hljs-number">0</span> <span class="hljs-comment"># 初始化信息增益</span><br>    axis = -<span class="hljs-number">1</span> <span class="hljs-comment"># 初始化最佳切分列，</span><br>标签列<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dataSet.shape[<span class="hljs-number">1</span>]-<span class="hljs-number">1</span>): <span class="hljs-comment"># 对特征的每⼀列进⾏</span><br>    循环<br>    levels= dataSet.iloc[:,i].value_counts().index <span class="hljs-comment"># 提取出当前列的所有</span><br>    取值<br>    ents = <span class="hljs-number">0</span> <span class="hljs-comment"># 初始化⼦节点的信息</span><br>    熵<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> levels: <span class="hljs-comment"># 对当前列的每⼀个取值进⾏循环</span><br>        childSet = dataSet[dataSet.iloc[:,i]==j] <span class="hljs-comment"># 某⼀个⼦节点的dataframe</span><br>        ent = calEnt(childSet) <span class="hljs-comment"># 计算某⼀个⼦节点的信息熵</span><br>        ents += (childSet.shape[<span class="hljs-number">0</span>]/dataSet.shape[<span class="hljs-number">0</span>])*ent <span class="hljs-comment"># 计算当前列的信息熵</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;第<span class="hljs-subst">&#123;i&#125;</span>列的信息熵为<span class="hljs-subst">&#123;ents&#125;</span>&#x27;</span>)<br>        infoGain = baseEnt-ents <span class="hljs-comment"># 计算当前列的信息增益</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;第<span class="hljs-subst">&#123;i&#125;</span>列的信息增益为<span class="hljs-subst">&#123;infoGain&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">if</span> (infoGain &gt; bestGain):<br>            bestGain = infoGain <span class="hljs-comment"># 选择最⼤信息增益</span><br>            axis = i <span class="hljs-comment"># 最⼤信息增益所在列</span><br>            的索引<br>            <span class="hljs-keyword">return</span> axis<br>        <br><span class="hljs-comment"># bestSplit(dataSet) # 返回的结果为0，即选择第0列来切分数据集</span><br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数功能：按照给定的列划分数据集</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集</span><br><span class="hljs-string"> axis：指定的列索引</span><br><span class="hljs-string"> value：指定的属性值</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> redataSet：按照指定列索引和属性值切分后的数据集</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mySplit</span>(<span class="hljs-params">dataSet,axis,value</span>):</span><br>    col = dataSet.columns[axis]<br>    redataSet = dataSet.loc[dataSet[col]==value,:].drop(col,axis=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> redataSet<br><br><br><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">函数功能：基于最⼤信息增益切分数据集，递归构建决策树</span><br><span class="hljs-string">参数说明：</span><br><span class="hljs-string"> dataSet：原始数据集(最有⼀列是标签)</span><br><span class="hljs-string">返回：</span><br><span class="hljs-string"> myTree：字典形式的树</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createTree</span>(<span class="hljs-params">dataSet</span>):</span><br>    featlist = <span class="hljs-built_in">list</span>(dataSet.columns) <span class="hljs-comment"># 提取出数据集所有的列</span><br>    classlist = dataSet.iloc[:,-<span class="hljs-number">1</span>].value_counts() <span class="hljs-comment"># 获取最后⼀列类标签</span><br>    <span class="hljs-comment"># 判断最多标签数⽬是否等于数据集⾏数，或者数据集是否只有⼀列</span><br>    <span class="hljs-keyword">if</span> classlist[<span class="hljs-number">0</span>]==dataSet.shape[<span class="hljs-number">0</span>] <span class="hljs-keyword">or</span> dataSet.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> classlist.index[<span class="hljs-number">0</span>] <span class="hljs-comment"># 如果是，返回类标签</span><br>    axis = bestSplit(dataSet) <span class="hljs-comment"># 确定出当前最佳切分列的索引</span><br>    bestfeat = featlist[axis] <span class="hljs-comment"># 获取该索引对应的特征</span><br>    myTree = &#123;bestfeat:&#123;&#125;&#125; <span class="hljs-comment"># 采⽤字典嵌套的⽅式存储树信息</span><br>    <span class="hljs-keyword">del</span> featlist[axis] <span class="hljs-comment"># 删除当前特征</span><br>    valuelist = <span class="hljs-built_in">set</span>(dataSet.iloc[:,axis]) <span class="hljs-comment"># 提取最佳切分列所有属性值</span><br>    <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> valuelist: <span class="hljs-comment"># 对每⼀个属性值递归建</span><br>        树<br>        myTree[bestfeat][value] = createTree(mySplit(dataSet,axis,value))<br>        <span class="hljs-keyword">return</span> myTree<br>    <br>myTree = createTree(dataSet)<br>myTree<br><br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
      <category>模型选择</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>决策树</tag>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python机器学习实用库（一）</title>
    <link href="/2021/11/02/%E3%80%90Data%20Science%E3%80%91Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2021/11/02/%E3%80%90Data%20Science%E3%80%91Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%94%A8%E5%BA%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th>库名</th><th>功能</th></tr></thead><tbody><tr><td>Numpy(SciPy) / Pandas / Matplotlib</td><td>三剑客，统计基础</td></tr><tr><td>Sklearn / Statsmodels / Eli5</td><td>结构化ML常用框架</td></tr><tr><td>Tqdm / Re / Gc / Os / Warning / Time</td><td>常用的基础辅助库</td></tr><tr><td>Imbalanced-learn / Missingpy</td><td>特征工程常用库</td></tr><tr><td>Seaborn / Pyecharts / Plotly</td><td>可视化常用库</td></tr><tr><td>Pandas_profiling</td><td>数据分析看板库</td></tr><tr><td>Optuna / Scikit-optimize / Skopt / HyperOpt</td><td>贝叶斯优化库</td></tr><tr><td>Pytorch / Tersonflow / Keras / Theano / Sonnet</td><td>DL常用框架</td></tr><tr><td>Xgboost / Catboost / Lightgbm / Ngboost</td><td>四大集成树模型</td></tr><tr><td>PyFlux</td><td>时序模型</td></tr><tr><td>Pycaret / MLJAR AutoML</td><td>AutoML库</td></tr><tr><td>Featuretools</td><td>特征构造库</td></tr><tr><td>Faker</td><td>假数据生成</td></tr><tr><td>Scrapy</td><td>数据采集</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Data Science】常用统计指标</title>
    <link href="/2021/10/09/%E3%80%90Data%20Science%E3%80%91%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%A0%87/"/>
    <url>/2021/10/09/%E3%80%90Data%20Science%E3%80%91%E5%B8%B8%E7%94%A8%E7%BB%9F%E8%AE%A1%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<h2 id="基础指标"><a href="#基础指标" class="headerlink" title="基础指标"></a>基础指标</h2><h3 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a>平均数</h3><ul><li><p>任一数据的变动都会引起该数值的变动，<strong>受极端值影响较大</strong>。</p></li><li><p>通常而言我们不会使用平均数来填充缺失值。</p><ul><li>均值会因偏态而无法准确反应样本实际情况，使用要慎重。</li></ul></li></ul><ul><li>平均数需要有实际意义，注意辛普森悖论。</li></ul><h3 id="众数"><a href="#众数" class="headerlink" title="众数"></a>众数</h3><ul><li><p>分类问题中常用，偷懒时，可用于离散型缺失值填充。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">series.fillna(series.mode()[<span class="hljs-number">0</span>],inplace = <span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure></li><li><p>众数也即频数，频数通常在特征构造中时常使用，作为一种特殊的编码(Frequence Encoder）。</p></li></ul><h3 id="百分位数"><a href="#百分位数" class="headerlink" title="百分位数"></a>百分位数</h3><ul><li><p>中位数：连续问题中常用此来观测数据的偏态，且可用于<strong>对完全随机缺失的连续性特征进行填充</strong>。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">series.fillna(series.median().inplace = <span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure></li><li><p>其他百分位数</p><ul><li>分箱</li><li>异常值阈值</li></ul></li></ul><h3 id="相对数"><a href="#相对数" class="headerlink" title="相对数"></a>相对数</h3><ul><li>增量</li><li>比率</li></ul><h2 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h2><h3 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h3><h3 id="方差-Var"><a href="#方差-Var" class="headerlink" title="方差(Var)"></a>方差(Var)</h3><h3 id="标准差-Std"><a href="#标准差-Std" class="headerlink" title="标准差(Std)"></a>标准差(Std)</h3><h3 id="协方差-Cov"><a href="#协方差-Cov" class="headerlink" title="协方差(Cov)"></a>协方差(Cov)</h3><h3 id="相关系数-Corr"><a href="#相关系数-Corr" class="headerlink" title="相关系数(Corr)"></a>相关系数(Corr)</h3><h3 id="可决系数-R-2"><a href="#可决系数-R-2" class="headerlink" title="可决系数(R^2)"></a>可决系数(R^2)</h3><h3 id="离差平方和-SST"><a href="#离差平方和-SST" class="headerlink" title="离差平方和(SST)"></a>离差平方和(SST)</h3><h3 id="残差-组间-平方和-SSE"><a href="#残差-组间-平方和-SSE" class="headerlink" title="残差(组间)平方和(SSE)"></a>残差(组间)平方和(SSE)</h3><h3 id="回归-组内-平方和-SSR"><a href="#回归-组内-平方和-SSR" class="headerlink" title="回归(组内)平方和(SSR)"></a>回归(组内)平方和(SSR)</h3><h3 id="泛化误差三大组成——噪音-偏差-方差"><a href="#泛化误差三大组成——噪音-偏差-方差" class="headerlink" title="泛化误差三大组成——噪音\偏差\方差"></a>泛化误差三大组成——噪音\偏差\方差</h3><h2 id="分类问题——混淆矩阵（二分类为基础）"><a href="#分类问题——混淆矩阵（二分类为基础）" class="headerlink" title="分类问题——混淆矩阵（二分类为基础）"></a>分类问题——混淆矩阵（二分类为基础）</h2><h3 id="TP、FP、TN、FN"><a href="#TP、FP、TN、FN" class="headerlink" title="TP、FP、TN、FN"></a>TP、FP、TN、FN</h3><h3 id="TPR、FPR、TNR、FNR"><a href="#TPR、FPR、TNR、FNR" class="headerlink" title="TPR、FPR、TNR、FNR"></a>TPR、FPR、TNR、FNR</h3><h3 id="第一类错误、第二类错误、Accuracy、Precision、Recall"><a href="#第一类错误、第二类错误、Accuracy、Precision、Recall" class="headerlink" title="第一类错误、第二类错误、Accuracy、Precision、Recall"></a>第一类错误、第二类错误、Accuracy、Precision、Recall</h3><h3 id="f1-score、fn-score"><a href="#f1-score、fn-score" class="headerlink" title="f1-score、fn-score"></a>f1-score、fn-score</h3><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><h3 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h3><h2 id="数据分布"><a href="#数据分布" class="headerlink" title="数据分布"></a>数据分布</h2><h3 id="显著性、置信度、置信区间"><a href="#显著性、置信度、置信区间" class="headerlink" title="显著性、置信度、置信区间"></a>显著性、置信度、置信区间</h3><h3 id="F、t、N、卡方检验值"><a href="#F、t、N、卡方检验值" class="headerlink" title="F、t、N、卡方检验值"></a>F、t、N、卡方检验值</h3><h3 id="熵、交叉熵、KL散度"><a href="#熵、交叉熵、KL散度" class="headerlink" title="熵、交叉熵、KL散度"></a>熵、交叉熵、KL散度</h3><h3 id="极大似然估计量与最大后验估计量"><a href="#极大似然估计量与最大后验估计量" class="headerlink" title="极大似然估计量与最大后验估计量"></a>极大似然估计量与最大后验估计量</h3>]]></content>
    
    
    <categories>
      
      <category>数据科学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>评价指标</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
